{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 18:15:08.801996: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_checkpoint = 'bert-base-cased'\n",
    "model_name = 'bert-finetuned-cadec'\n",
    "root_txt = 'data/cadec/text/'\n",
    "root_ann = 'data/cadec/original/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['O', 'B-ADR', 'I-ADR', 'B-Drug', 'I-Drug', 'B-Finding', \n",
    "               'I-Finding', 'B-Disease', 'I-Disease', 'B-Symptom', 'I-Symptom']\n",
    "id2label = {i: l for i, l in enumerate(label_names)}\n",
    "label2id = {l: i for i, l in id2label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(fn):\n",
    "    med, i = re.findall(r'(\\w+)\\.(\\d+)\\.txt', fn)[0]\n",
    "    i = int(i)\n",
    "\n",
    "    with open(os.path.join(root_txt, fn), 'r') as infile:\n",
    "        text = infile.readlines()\n",
    "        text = ''.join(text)\n",
    "    with open(os.path.join(root_ann, fn.replace('txt', 'ann')), 'r') as infile:\n",
    "        annotations = infile.readlines()\n",
    "        annotations = [l.strip() for l in annotations if not l.startswith('#')]\n",
    "    return i, med, text, annotations\n",
    "    \n",
    "\n",
    "def parse_annotations(lines):\n",
    "    annots = {}\n",
    "    for i in range(len(lines)):\n",
    "        annots[i] = {}\n",
    "        entity = re.findall(r'(Finding|ADR|Drug|Disease|Symptom) ([\\d; ]+)\\t(.*)$', \n",
    "                            lines[i])[0]\n",
    "        annots[i]['ner'] = entity[0]\n",
    "        boundaries = entity[1].split(';')\n",
    "        boundaries = [[int(bb) for bb in b.split()] for b in boundaries]\n",
    "        annots[i]['boundaries'] = boundaries\n",
    "        annots[i]['text'] = entity[2]\n",
    "    return annots\n",
    "\n",
    "\n",
    "def get_current_annot(annots, idx, start):\n",
    "    if idx >= len(annots):\n",
    "        return idx - 1\n",
    "    boundaries = annots[idx]['boundaries']\n",
    "    if start > boundaries[-1][-1]:\n",
    "        return get_current_annot(annots, idx+1, start)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def print_tags_tokens(data):\n",
    "    tokens = data['tokens']\n",
    "    tags = data['ner']\n",
    "    line1 = \"\"\n",
    "    line2 = \"\"\n",
    "    for word, label in zip(tokens, tags):\n",
    "        max_length = max(len(word), len(label))\n",
    "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "        line2 += label + \" \" * (max_length - len(label) + 1)\n",
    "    print(line1)\n",
    "    print(line2)\n",
    "\n",
    "\n",
    "def get_IOB_tags(text: str, annotations: dict, map_tags_to_int=False):\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "    if len(annotations) == 0:\n",
    "        return tokens, [0 for _ in tokens]\n",
    "    offset = 0\n",
    "    idx = 0\n",
    "    tags = []\n",
    "    text_tmp = text\n",
    "    for token in tokens:\n",
    "        span = np.asarray(re.search(re.escape(token), text_tmp).span())\n",
    "        idx = get_current_annot(annotations, idx, span[0] + offset)\n",
    "        boundaries = annotations[idx]['boundaries']\n",
    "        found = False\n",
    "        for i, (start, end) in enumerate(boundaries):\n",
    "            if (span[0] + offset >= start) and (span[1] + offset <= end):\n",
    "                prefix = 'B-'\n",
    "                if i > 0 or span[0] + offset > start:\n",
    "                    prefix = 'I-'\n",
    "                tags.append(prefix + annotations[idx]['ner'])\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            tags.append('O')\n",
    "        offset += span[1]\n",
    "        text_tmp = text_tmp[span[1]:]\n",
    "    if map_tags_to_int:\n",
    "        tags = [label2id[tag] for tag in tags]\n",
    "    return tokens, tags\n",
    "\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['tokens'], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples['ner']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    \n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing data into the desired format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "for fn in os.listdir(root_txt):\n",
    "    i, med, text, annotations = read_files(fn)\n",
    "    annots = parse_annotations(annotations)\n",
    "    tokens, tags = get_IOB_tags(text, annots, map_tags_to_int=True)\n",
    "    if med not in data.keys():\n",
    "        data[med] = {}\n",
    "    data[med][i] = {\n",
    "        'tokens': tokens, 'ner': tags\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {}\n",
    "idx = 0\n",
    "train_pct = 0.64\n",
    "val_pct = 0.16\n",
    "test_pct = 0.2\n",
    "for v in data.values():\n",
    "    for vv in v.values():\n",
    "        raw_data[idx] = vv\n",
    "        idx += 1\n",
    "\n",
    "indices = np.arange(len(raw_data))\n",
    "np.random.shuffle(indices)\n",
    "train_idx = indices[:int(train_pct * len(indices))]\n",
    "val_idx = indices[int(train_pct * len(indices)):int((train_pct+val_pct) * len(indices))]\n",
    "test_idx = indices[-int(test_pct * len(indices)):]\n",
    "\n",
    "data_split = {'train': {}, 'val': {}, 'test': {}}\n",
    "for split, indices in zip(('train', 'val', 'test'), (train_idx, val_idx, test_idx)):\n",
    "    for idx in indices:\n",
    "        for k in raw_data[idx].keys():\n",
    "            if k not in data_split[split]:\n",
    "                data_split[split][k] = []\n",
    "            data_split[split][k].append(raw_data[idx][k])\n",
    "\n",
    "datasets = {k: Dataset.from_dict(data_split[k]) for k in data_split.keys()}\n",
    "datasets = DatasetDict(datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = datasets['train']['ner']\n",
    "ner = [tag for sentence in ner for tag in sentence]\n",
    "tags, counts = np.unique(ner, return_counts=True)\n",
    "df_tags = pd.DataFrame(index=tags, data=counts, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(label_names, \n",
    "       df_tags.loc[[label2id[l] for l in label_names]].counts.values, \n",
    "       zorder=2)\n",
    "ax.set_yscale('log')\n",
    "ax.grid(axis='y', zorder=1)\n",
    "ax.set_ylim(1, 1e5)\n",
    "plt.xticks(rotation=45)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8460bf62604d4728a2b57d2500a01b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ec3bb4b2b545b481824c3380f783e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a09710521384971b477109045a9de99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_checkpoint)\n",
    "\n",
    "data_tokenized = datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=datasets['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "metric = evaluate.load('seqeval')\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    base_model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    model_name,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    #push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=data_tokenized[\"train\"],\n",
    "    eval_dataset=data_tokenized[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7ff83f5b324da1a9eb3deca960b3f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 6.84 GB, other allocations: 6.74 GB, max allowed: 13.57 GB). Tried to allocate 10.57 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:2122\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2120\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2123\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2124\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   2125\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   2126\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2127\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:2474\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2471\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2473\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2474\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2477\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2479\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2480\u001b[0m ):\n\u001b[1;32m   2481\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2482\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:3572\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3571\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3572\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs, num_items_in_batch\u001b[38;5;241m=\u001b[39mnum_items_in_batch)\n\u001b[1;32m   3574\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3578\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:3625\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3623\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3624\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3625\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m   3626\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1862\u001b[0m, in \u001b[0;36mBertForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1860\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1862\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(\n\u001b[1;32m   1863\u001b[0m     input_ids,\n\u001b[1;32m   1864\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1865\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   1866\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1867\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1868\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1869\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1870\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1871\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1872\u001b[0m )\n\u001b[1;32m   1874\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1876\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1143\u001b[0m     embedding_output,\n\u001b[1;32m   1144\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   1145\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1146\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1147\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m   1148\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1149\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1150\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1151\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1152\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1153\u001b[0m )\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m    696\u001b[0m         hidden_states,\n\u001b[1;32m    697\u001b[0m         attention_mask,\n\u001b[1;32m    698\u001b[0m         layer_head_mask,\n\u001b[1;32m    699\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    700\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    701\u001b[0m         past_key_value,\n\u001b[1;32m    702\u001b[0m         output_attentions,\n\u001b[1;32m    703\u001b[0m     )\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[1;32m    586\u001b[0m         hidden_states,\n\u001b[1;32m    587\u001b[0m         attention_mask,\n\u001b[1;32m    588\u001b[0m         head_mask,\n\u001b[1;32m    589\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    590\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[1;32m    591\u001b[0m     )\n\u001b[1;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    516\u001b[0m         hidden_states,\n\u001b[1;32m    517\u001b[0m         attention_mask,\n\u001b[1;32m    518\u001b[0m         head_mask,\n\u001b[1;32m    519\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    520\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    521\u001b[0m         past_key_value,\n\u001b[1;32m    522\u001b[0m         output_attentions,\n\u001b[1;32m    523\u001b[0m     )\n\u001b[1;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:409\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(current_states))\n\u001b[0;32m--> 409\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(current_states))\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention:\n\u001b[1;32m    411\u001b[0m         key_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m0\u001b[39m], key_layer], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 6.84 GB, other allocations: 6.74 GB, max allowed: 13.57 GB). Tried to allocate 10.57 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "#trainer.push_to_hub(commit_message=\"Training complete\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
